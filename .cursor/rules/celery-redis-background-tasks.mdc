---
description: Celery background task processing with Redis and VK API integration
globs: **/tasks*.py, **/celery*.py, **/worker*.py, **/scheduler*.py
alwaysApply: false
---

---
description: "Celery background task processing with Redis and VK API integration"
globs: ["**/workers/**/*.py", "**/tasks/**/*.py", "**/celery*.py", "**/cache.py"]
alwaysApply: false
---

# Celery & Redis Background Tasks

–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å Celery, Redis –∏ —Ñ–æ–Ω–æ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –í–ö–æ–Ω—Ç–∞–∫—Ç–µ.

## Celery Configuration üöÄ

### 1. Modern Celery Setup
```python
# workers/celery.py
from celery import Celery, Task
from celery.signals import worker_init, worker_shutdown
import asyncio
from app.config import settings

class AsyncTask(Task):
    def __call__(self, *args, **kwargs):
        if asyncio.iscoroutinefunction(self.run):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                return loop.run_until_complete(self.run(*args, **kwargs))
            finally:
                loop.close()
        return super().__call__(*args, **kwargs)

celery_app = Celery(
    "vk_monitor",
    task_cls=AsyncTask,
    broker=settings.REDIS_URL,
    backend=settings.REDIS_URL,
    include=['app.workers.vk_scanner_tasks', 'app.workers.notification_tasks']
)

celery_app.conf.update(
    timezone='UTC',
    enable_utc=True,
    
    task_routes={
        'app.workers.vk_scanner_tasks.*': {'queue': 'vk_scanning'},
        'app.workers.notification_tasks.*': {'queue': 'notifications'},
    },
    
    worker_prefetch_multiplier=1,
    task_acks_late=True,
    task_serializer='json',
    result_serializer='json',
    task_compression='gzip',
    
    beat_schedule={
        'scan-active-groups': {
            'task': 'app.workers.vk_scanner_tasks.scan_active_groups',
            'schedule': 300.0,  # –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
        },
        'cleanup-old-comments': {
            'task': 'app.workers.cleanup_tasks.cleanup_old_comments',
            'schedule': 3600.0,  # –∫–∞–∂–¥—ã–π —á–∞—Å
        }
    },
)
```

### 2. VK Scanning Tasks
```python
# workers/vk_scanner_tasks.py
from celery import group
from typing import List, Dict, Any
from datetime import datetime, timedelta

from app.workers.celery import celery_app
from app.database import db_manager
from app.services.group_service import GroupService
from app.core.vk_scanner import VKScanner

@celery_app.task(bind=True, max_retries=3, default_retry_delay=60)
async def scan_group_comments(self, group_id: int, keywords: List[str]) -> Dict[str, Any]:
    try:
        async with db_manager.session_maker() as session:
            group_service = GroupService(session)
            
            group = await group_service.get_by_id(group_id)
            if not group or not group.is_active:
                return {"status": "skipped", "reason": "Group inactive"}
            
            scanner = VKScanner()
            scan_result = await scanner.scan_group_comments(
                group_id=group.vk_group_id,
                keywords=keywords,
                limit=100
            )
            
            return {
                "status": "success",
                "group_id": group_id,
                "comments_found": len(scan_result.get('comments', [])),
                "posts_scanned": scan_result.get('posts_scanned', 0)
            }
    except Exception as exc:
        raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries))

@celery_app.task
async def scan_active_groups() -> Dict[str, Any]:
    async with db_manager.session_maker() as session:
        group_service = GroupService(session)
        active_groups = await group_service.get_active_groups()
        
        if not active_groups:
            return {"status": "no_active_groups", "count": 0}
        
        scan_tasks = []
        for group in active_groups:
            keywords = [kw.keyword for kw in group.keywords if kw.is_active]
            if keywords:
                task = scan_group_comments.s(group.id, keywords)
                scan_tasks.append(task)
        
        if scan_tasks:
            job = group(scan_tasks)
            result = job.apply_async()
            return {
                "status": "scheduled",
                "groups_count": len(active_groups),
                "tasks_created": len(scan_tasks)
            }
        
        return {"status": "no_keywords", "groups_count": len(active_groups)}
```

## Redis Caching üîÑ

### 3. Redis Configuration
```python
# core/cache.py
import json
import redis.asyncio as redis
from typing import Any, Optional
from app.config import settings

class RedisManager:
    def __init__(self):
        self.redis = redis.from_url(
            settings.REDIS_URL,
            max_connections=20,
            retry_on_timeout=True,
            health_check_interval=30,
        )
    
    async def close(self):
        await self.redis.close()

redis_manager = RedisManager()

class CacheManager:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
    
    async def get(self, key: str, namespace: str = "cache") -> Optional[Any]:
        cache_key = f"{namespace}:{key}"
        try:
            value = await self.redis.get(cache_key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            print(f"Cache get error: {e}")
            return None
    
    async def set(self, key: str, value: Any, expire: int = 3600, namespace: str = "cache") -> bool:
        cache_key = f"{namespace}:{key}"
        try:
            serialized_value = json.dumps(value, default=str)
            return await self.redis.set(cache_key, serialized_value, ex=expire)
        except Exception as e:
            print(f"Cache set error: {e}")
            return False
    
    async def delete(self, key: str, namespace: str = "cache") -> bool:
        cache_key = f"{namespace}:{key}"
        try:
            result = await self.redis.delete(cache_key)
            return bool(result)
        except Exception as e:
            print(f"Cache delete error: {e}")
            return False

cache = CacheManager(redis_manager.redis)
```

### 4. Rate Limiting
```python
# core/rate_limiter.py
from datetime import datetime, timedelta
from typing import Optional

class RateLimiter:
    def __init__(self, cache_manager):
        self.cache = cache_manager
    
    async def is_allowed(
        self, 
        identifier: str, 
        limit: int, 
        window_seconds: int,
        namespace: str = "rate_limit"
    ) -> tuple[bool, dict]:
        current_time = datetime.utcnow()
        window_start = current_time - timedelta(seconds=window_seconds)
        
        key = f"{identifier}:{window_seconds}:{limit}"
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ timestamps
        timestamps = await self.cache.get(key, namespace) or []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–∞—Ä—ã–µ timestamps
        valid_timestamps = [
            ts for ts in timestamps 
            if datetime.fromisoformat(ts) > window_start
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
        if len(valid_timestamps) >= limit:
            oldest_in_window = min(valid_timestamps, key=lambda x: datetime.fromisoformat(x))
            retry_after = (datetime.fromisoformat(oldest_in_window) + timedelta(seconds=window_seconds) - current_time).total_seconds()
            
            return False, {
                "remaining": 0,
                "retry_after": max(0, retry_after)
            }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π timestamp
        valid_timestamps.append(current_time.isoformat())
        await self.cache.set(key, valid_timestamps, expire=window_seconds + 1, namespace=namespace)
        
        remaining = limit - len(valid_timestamps)
        return True, {"remaining": remaining, "retry_after": 0}

rate_limiter = RateLimiter(cache)
```

## VK API Integration üîó

### 5. VK Scanner with Rate Limiting
```python
# core/vk_scanner.py
import asyncio
from typing import List, Dict, Any, Optional
import aiohttp
from app.core.rate_limiter import rate_limiter
from app.config import settings

class VKScanner:
    def __init__(self):
        self.access_token = settings.VK_ACCESS_TOKEN
        self.api_version = "5.131"
        self.base_url = "https://api.vk.com/method"
    
    async def _make_api_request(self, method: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º rate limit (VK API: 3 –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥—É)
        is_allowed, limit_info = await rate_limiter.is_allowed(
            identifier="vk_api",
            limit=3,
            window_seconds=1,
            namespace="vk_rate_limit"
        )
        
        if not is_allowed:
            retry_after = limit_info.get("retry_after", 1)
            await asyncio.sleep(retry_after)
        
        params.update({
            "access_token": self.access_token,
            "v": self.api_version
        })
        
        url = f"{self.base_url}/{method}"
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, params=params) as response:
                    data = await response.json()
                    
                    if "error" in data:
                        error_code = data["error"]["error_code"]
                        if error_code == 6:  # Too many requests
                            await asyncio.sleep(1)
                            return await self._make_api_request(method, params)
                        else:
                            raise Exception(f"VK API error {error_code}")
                    
                    return data.get("response")
                    
            except aiohttp.ClientError as e:
                raise Exception(f"HTTP request failed: {e}")
    
    async def scan_group_comments(self, group_id: int, keywords: List[str], limit: int = 100) -> Dict[str, Any]:
        # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã –≥—Ä—É–ø–ø—ã
        posts_response = await self._make_api_request(
            "wall.get",
            {"owner_id": f"-{group_id}", "count": 20, "filter": "owner"}
        )
        
        if not posts_response or not posts_response.get("items"):
            return {"comments": [], "posts_scanned": 0}
        
        all_comments = []
        posts_scanned = 0
        
        for post in posts_response["items"]:
            if len(all_comments) >= limit:
                break
                
            post_id = post["id"]
            posts_scanned += 1
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫ –ø–æ—Å—Ç—É
            comments_response = await self._make_api_request(
                "wall.getComments",
                {"owner_id": f"-{group_id}", "post_id": post_id, "count": 100}
            )
            
            if not comments_response or not comments_response.get("items"):
                continue
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            for comment in comments_response["items"]:
                if len(all_comments) >= limit:
                    break
                
                comment_text = comment.get("text", "").lower()
                matched_keyword = None
                
                for keyword in keywords:
                    if keyword.lower() in comment_text:
                        matched_keyword = keyword
                        break
                
                if matched_keyword:
                    comment_data = {
                        "id": comment["id"],
                        "post_id": post_id,
                        "from_id": comment["from_id"],
                        "text": comment["text"],
                        "date": comment["date"],
                        "matched_keyword": matched_keyword
                    }
                    all_comments.append(comment_data)
        
        return {"comments": all_comments, "posts_scanned": posts_scanned}
```

## Cleanup Tasks üßπ

### 6. Maintenance Tasks
```python
# workers/cleanup_tasks.py
from datetime import datetime, timedelta
from sqlalchemy import delete
from app.workers.celery import celery_app
from app.database import db_manager
from app.models.comment import Comment

@celery_app.task
async def cleanup_old_comments(days_to_keep: int = 30) -> Dict[str, Any]:
    cutoff_date = datetime.utcnow() - timedelta(days=days_to_keep)
    
    async with db_manager.session_maker() as session:
        result = await session.execute(
            delete(Comment).where(Comment.created_at < cutoff_date)
        )
        
        deleted_count = result.rowcount
        await session.commit()
        
        return {
            "status": "success",
            "deleted_comments": deleted_count,
            "cutoff_date": cutoff_date.isoformat()
        }

@celery_app.task
async def cleanup_redis_cache() -> Dict[str, Any]:
    redis_client = redis_manager.redis
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–∏ —Å –Ω–∞—à–∏–º namespace
    pattern = "vk_monitor:*"
    keys = await redis_client.keys(pattern)
    
    expired_keys = 0
    total_keys = len(keys)
    
    for key in keys:
        ttl = await redis_client.ttl(key)
        if ttl == -1:  # –ö–ª—é—á –±–µ–∑ TTL
            await redis_client.expire(key, 86400)
        elif ttl == -2:  # –ö–ª—é—á —É–∂–µ –∏—Å—Ç–µ–∫
            expired_keys += 1
    
    return {
        "status": "success",
        "total_keys": total_keys,
        "expired_keys": expired_keys
    }
```

## Docker Configuration üê≥

### 7. Docker Compose for Celery
```yaml
# docker-compose.celery.yml
version: "3.9"

services:
  celery-worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    command: celery -A app.workers.celery worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - VK_ACCESS_TOKEN=${VK_ACCESS_TOKEN}
    depends_on:
      - redis
      - postgres
    restart: unless-stopped

  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile.worker
    command: celery -A app.workers.celery beat --loglevel=info
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - redis
      - postgres
    restart: unless-stopped

  celery-flower:
    build:
      context: .
      dockerfile: Dockerfile.worker
    command: celery -A app.workers.celery flower --port=5555
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=${REDIS_URL}
    depends_on:
      - redis
    restart: unless-stopped
```

## Best Practices Checklist ‚úÖ

### Task Design:
- [ ] Tasks –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω—ã
- [ ] Retry logic —Å exponential backoff
- [ ] Timeout –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
- [ ] Proper error handling

### Performance:
- [ ] Connection pooling –¥–ª—è Redis
- [ ] Rate limiting –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö API
- [ ] Batch operations –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
- [ ] Queue routing –Ω–∞—Å—Ç—Ä–æ–µ–Ω

### Monitoring:
- [ ] Health checks –¥–ª—è –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
- [ ] Task duration monitoring
- [ ] Resource usage tracking
- [ ] Alert –Ω–∞ failed tasks

### Security:
- [ ] Sensitive data –Ω–µ –≤ task args
- [ ] Secure serialization
- [ ] Rate limiting –∑–∞—â–∏—Ç–∞
- [ ] Environment variables –¥–ª—è secrets

### Deployment:
- [ ] Graceful shutdown handling
- [ ] Auto-restart policies
- [ ] Resource limits —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
- [ ] Logging centralized
